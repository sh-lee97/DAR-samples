<!DOCTYPE html>
<html lang="en">
<head>
  <style>
    div {
      max-width: 1000px;
      min-width: 600px;
    }
    table, th, td {
      border: 1px solid black;
    }
  </style>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Document</title>
</head>
<body>
  <center>
  <h1> Differentiable Artificial Reverberation  </h1>

    Sungho Lee, Hyeong-Seok Choi, and Kyogu Lee <br>
    Music and Audio Research Group, Seoul National University, Seoul, Republic of Korea <br>
    <br>
    <b>ABSTRACT</b>
  </center>

Artificial reverberation (AR) models play a central role in various audio applications. Therefore, estimating the AR model parameters (ARPs) of a target reverberation is a crucial task. Although a few recent deep-learning-based approaches have shown promising performance, their non-end-to-end training scheme prevents them from fully exploiting the potential of deep neural networks. This motivates to introduce differentiable artificial reverberation (DAR) models which allows loss gradients to be back-propagated end-to-end. However, implementing the AR models with their difference equations "as is" in the deep-learning framework severely bottlenecks the training speed when executed with a parallel processor like GPU due to their infinite impulse response (IIR) components. We tackle this problem by replacing the IIR filters with finite impulse response (FIR) approximations with the frequency-sampling method (FSM). Using the FSM, we implement three DAR models---differentiable Filtered Velvet Noise (FVN), Advanced Filtered Velvet Noise (AFVN), and Feedback Delay Network (FDN). For each AR model, we train its ARP estimation networks for analysis-synthesis (RIR-to-ARP) and blind estimation (reverberant-speech-to-ARP) task in an end-to-end manner with its DAR model counterpart. Experiment results show that the proposed method achieves consistent performance improvement over the non-end-to-end approaches in both objective metrics and subjective listening test results. 
  
  <center>
  <br>
  <img src="Final.png" alt="Framework">
  <br>
  <br>
  Proposed Framework.
  <br>
  <br>
  <h3> Audio Examples </h3>
  <a href="fvn.html">Filtered Velvet Noise (FVN)</a><br>
  <a href="afvn.html">Advanced Filtered Velvet Noise (AFVN)</a><br>
  <a href="fdn.html">Feedback Delay Network (FDN)</a><br>
  <a href="dnn.html">DNN Decoder Baseline (DNN)</a>
  </center>

</body>
</html>
